{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generate_contrastive_claims.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGAnSJnS8g-n",
        "outputId": "4e01362e-592a-4be9-bb74-b0b131cdcb61"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "dir = \"/content/drive/My Drive/misinformation-NLP/\"\n",
        "os.listdir(dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['yake_reddit.csv',\n",
              " 'ILM_fake_news_token_72hours',\n",
              " 'CORD19-GPT',\n",
              " 'scifact',\n",
              " 'reddit_BERT_SCIFACT.csv',\n",
              " '.ipynb_checkpoints',\n",
              " 'results',\n",
              " 'fake-news-corpus',\n",
              " 'reddit-data',\n",
              " 'masked_claims',\n",
              " 'models',\n",
              " 'tmp',\n",
              " 'data_for_github',\n",
              " 'AMT_evidence_batches']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "J1a4D0i290zs",
        "outputId": "79d9d7db-9f10-4654-e89b-9114bfef9e7b"
      },
      "source": [
        "claims = pd.read_csv('/content/drive/MyDrive/misinformation-NLP/reddit-data/pushshift_additional_1k.csv')[['title', 'flair', 'source_url']]\n",
        "claims['claim'] = claims['title'].apply(lambda x: \" \".join(x.split(\" \")))\n",
        "claims.drop_duplicates(subset=['claim'], inplace=True)\n",
        "claims"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>flair</th>\n",
              "      <th>source_url</th>\n",
              "      <th>claim</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Air Pollution Is Associated with COVID-19 Inci...</td>\n",
              "      <td>General</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7...</td>\n",
              "      <td>Air Pollution Is Associated with COVID-19 Inci...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Fauci says Trump policies very likely led to d...</td>\n",
              "      <td>General</td>\n",
              "      <td>https://www.scmp.com/news/world/united-states-...</td>\n",
              "      <td>Fauci says Trump policies very likely led to d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Boris Johnson says U.K. coronavirus variant ma...</td>\n",
              "      <td>Press Release</td>\n",
              "      <td>https://www.washingtonpost.com/world/europe/uk...</td>\n",
              "      <td>Boris Johnson says U.K. coronavirus variant ma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>New Uk variant may be more deadly</td>\n",
              "      <td>Press Release</td>\n",
              "      <td>https://www.bbc.co.uk/news/health-55768627</td>\n",
              "      <td>New Uk variant may be more deadly</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>New Uk variant May be more deadly</td>\n",
              "      <td>General</td>\n",
              "      <td>https://www.bbc.co.uk/news/health-55768627l</td>\n",
              "      <td>New Uk variant May be more deadly</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550</th>\n",
              "      <td>42 confirmed cases of coronavirus in India ; I...</td>\n",
              "      <td>General</td>\n",
              "      <td>https://www.emedinexus.com/post/16824/?utm_sou...</td>\n",
              "      <td>42 confirmed cases of coronavirus in India ; I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>551</th>\n",
              "      <td>Canada 's Military Ordered to Begin Preparing ...</td>\n",
              "      <td>Government Agency</td>\n",
              "      <td>https://www.7ummitmagazine.com/latest-news/202...</td>\n",
              "      <td>Canada 's Military Ordered to Begin Preparing ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>552</th>\n",
              "      <td>CDC Released Patient Who Tested Positive for C...</td>\n",
              "      <td>Social Impact</td>\n",
              "      <td>https://time.com/5793763/san-antonio-covid-19/</td>\n",
              "      <td>CDC Released Patient Who Tested Positive for C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>553</th>\n",
              "      <td>Iran says coronavirus has spread to several ci...</td>\n",
              "      <td>Government Agency</td>\n",
              "      <td>https://edition.cnn.com/asia/live-news/coronav...</td>\n",
              "      <td>Iran says coronavirus has spread to several ci...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>554</th>\n",
              "      <td>China Reports Smallest Number Of New COVID-19 ...</td>\n",
              "      <td>News Article</td>\n",
              "      <td>https://khn.org/morning-breakout/china-reports...</td>\n",
              "      <td>China Reports Smallest Number Of New COVID-19 ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>552 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 title  ...                                              claim\n",
              "0    Air Pollution Is Associated with COVID-19 Inci...  ...  Air Pollution Is Associated with COVID-19 Inci...\n",
              "1    Fauci says Trump policies very likely led to d...  ...  Fauci says Trump policies very likely led to d...\n",
              "2    Boris Johnson says U.K. coronavirus variant ma...  ...  Boris Johnson says U.K. coronavirus variant ma...\n",
              "3                    New Uk variant may be more deadly  ...                  New Uk variant may be more deadly\n",
              "4                    New Uk variant May be more deadly  ...                  New Uk variant May be more deadly\n",
              "..                                                 ...  ...                                                ...\n",
              "550  42 confirmed cases of coronavirus in India ; I...  ...  42 confirmed cases of coronavirus in India ; I...\n",
              "551  Canada 's Military Ordered to Begin Preparing ...  ...  Canada 's Military Ordered to Begin Preparing ...\n",
              "552  CDC Released Patient Who Tested Positive for C...  ...  CDC Released Patient Who Tested Positive for C...\n",
              "553  Iran says coronavirus has spread to several ci...  ...  Iran says coronavirus has spread to several ci...\n",
              "554  China Reports Smallest Number Of New COVID-19 ...  ...  China Reports Smallest Number Of New COVID-19 ...\n",
              "\n",
              "[552 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWqNTwaeRl6q"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJt3fcZoeqD3"
      },
      "source": [
        "##GENERATING KEYWORDS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SkQucdiZ81L-",
        "outputId": "14015288-1bed-4e13-bc11-9dea1f32a946"
      },
      "source": [
        "!!pip install --upgrade transformers\n",
        "!!git clone https://github.com/huggingface/transformers\n",
        "# !!pip install ./transformers\n",
        "!!pip install -r ./transformers/examples/requirements.txt\n",
        "!!pip install boto3\n",
        "!!pip install --upgrade pyarrow\n",
        "!!pip install --upgrade urllib3\n",
        "!!pip install --upgrade chardet\n",
        "import sys\n",
        "sys.path.insert(1, '/content/transformer-drg-style-transfer')\n",
        "!!git clone https://github.com/agaralabs/transformer-drg-style-transfer\n",
        "import csv\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "\n",
        "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
        "from pytorch_pretrained_bert.modeling import BertForSequenceClassification, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n",
        "#from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
        "from pytorch_pretrained_bert.optimization import BertAdam, warmup_linear\n",
        "\n",
        "from bertviz.bertviz import attention, visualization\n",
        "from bertviz.bertviz.pytorch_pretrained_bert import BertModel, BertTokenizer\n",
        "\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)\n",
        "\n",
        "# UNCASED MODEL\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "bert_classifier_model_dir =  '/content/drive/MyDrive/misinformation-NLP/models/BERT-SCIFACT-UNCASED' #\"/content/BERT-SCIFACT-UNCASED\" ## Path of BERT classifier model path\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "logger.info(\"device: {}, n_gpu {}\".format(device, n_gpu))\n",
        "\n",
        "## Model for performing Classification\n",
        "model_cls = BertForSequenceClassification.from_pretrained(bert_classifier_model_dir, num_labels=2)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) # replace scibert ?\n",
        "model_cls.to(device)\n",
        "model_cls.eval()\n",
        "\n",
        "## Model to get the attention weights of all the heads\n",
        "model = BertModel.from_pretrained(bert_classifier_model_dir)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "common_words=['is','are','was','were','has','have','had','a','an','the','this','that','these','those','there','how','i','we',\n",
        "             'he','she','it','they','them','their','his','him','her','us','our', 'and','in','my','your','you', 'will', 'shall']\n",
        "common_words_tokens = tokenizer.convert_tokens_to_ids(common_words)\n",
        "not_to_remove_ids = tokenizer.convert_tokens_to_ids([\"[CLS]\",\"[SEP]\", \".\", \"?\", \"!\"])\n",
        "not_to_remove_ids += common_words_tokens\n",
        "\n",
        "max_seq_len=70 # Maximum sequence length \n",
        "sm = torch.nn.Softmax(dim=-1) ## Softmax over the batch\n",
        "def run_attn_examples(input_sentences, layer, head, bs=128):\n",
        "    \"\"\"\n",
        "    Returns Attention weights for selected Layer and Head along with ids and tokens\n",
        "    of the input_sentence\n",
        "    \"\"\"\n",
        "    ids = []\n",
        "    ids_to_decode = [None for k in range(len(input_sentences))]\n",
        "    tokens_to_decode = [None for k in range(len(input_sentences))]\n",
        "    segment_ids = []\n",
        "    input_masks = []\n",
        "    attention_weights = [None for z in input_sentences]\n",
        "    ## BERT pre-processing\n",
        "    for j,sen in enumerate(tqdm(input_sentences)):\n",
        "        \n",
        "        text_tokens = tokenizer.tokenize(sen)\n",
        "        if len(text_tokens) >= max_seq_len-2:\n",
        "            text_tokens = text_tokens[:max_seq_len-4]\n",
        "        tokens = [\"[CLS]\"] + text_tokens + [\"[SEP]\"]\n",
        "        tokens_to_decode[j] = tokens\n",
        "        temp_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        ids_to_decode[j] = temp_ids\n",
        "        input_mask = [1] * len(temp_ids)\n",
        "        segment_id = [0] * len(temp_ids)\n",
        "        padding = [0] * (max_seq_len - len(temp_ids))\n",
        "        \n",
        "        temp_ids += padding\n",
        "        input_mask += padding\n",
        "        segment_id += padding\n",
        "        \n",
        "        ids.append(temp_ids)\n",
        "        input_masks.append(input_mask)\n",
        "        segment_ids.append(segment_id)\n",
        "    \n",
        "    # Convert Ids to Torch Tensors\n",
        "    ids = torch.tensor(ids) \n",
        "    segment_ids = torch.tensor(segment_ids)\n",
        "    input_masks = torch.tensor(input_masks)\n",
        "    \n",
        "    steps = len(ids) // bs\n",
        "    \n",
        "    for i in trange(steps+1):\n",
        "        if i == steps:\n",
        "            temp_ids = ids[i * bs : len(ids)]\n",
        "            temp_segment_ids = segment_ids[i * bs: len(ids)]\n",
        "            temp_input_masks = input_masks[i * bs: len(ids)]\n",
        "        else:\n",
        "            temp_ids = ids[i * bs : i * bs + bs]\n",
        "            temp_segment_ids = segment_ids[i * bs: i * bs + bs]\n",
        "            temp_input_masks = input_masks[i * bs: i * bs + bs]\n",
        "        \n",
        "        temp_ids = temp_ids.to(device)\n",
        "        temp_segment_ids = temp_segment_ids.to(device)\n",
        "        temp_input_masks = temp_input_masks.to(device)\n",
        "        with torch.no_grad():\n",
        "             _, _, attn = model(temp_ids, temp_segment_ids, temp_input_masks)\n",
        "        # Concate Attention weights\n",
        "        for j in range(len(attn[layer]['attn_probs'])):\n",
        "            attention_weights[i * bs + j] = (attn[layer]['attn_probs'][j][head][0]).to('cpu')\n",
        "    \n",
        "    return attention_weights, ids_to_decode, tokens_to_decode\n",
        "\n",
        "def prepare_data(aw, ids_to_decode, tokens_to_decode):\n",
        "    #out_sen = [None for i in range(len(aw))]\n",
        "    all_top_words = []\n",
        "    for i in trange(len(aw)):\n",
        "        topv, topi = aw[i].topk(len(ids_to_decode[i]))\n",
        "        #print(ids_to_decode[i].index(0))\n",
        "        topv, topi = aw[i].topk(ids_to_decode[i].index(0)) #aw[i].topk(k=2)\n",
        "        #topv, topi = aw[i].topk(k=2)\n",
        "        topi = topi.tolist()\n",
        "        topv = topv.tolist()\n",
        "        #print(topi)\n",
        "        #print(topv)\n",
        "        # print(i,train_0[i])\n",
        "        # print(tokens_to_decode[i])\n",
        "        # print(\"Original Top Indexes = {}\".format(topi))\n",
        "        topi = [topi[j] for j in range(len(topi)) if ids_to_decode[i][topi[j]] not in not_to_remove_ids] # remove noun and common words\n",
        "        #print(\"After removing Nouns = {}\".format(topi))\n",
        "        topi = [topi[j] for j in range(len(topi)) if \"##\" not in tokens_to_decode[i][topi[j]]] # Remove half words\n",
        "        #print(\"After removing Half-words = {}\".format(topi))\n",
        "        \n",
        "        top_words = []\n",
        "        printed=0 \n",
        "        for j in range(0, len(topi)):\n",
        "          \n",
        "          top_word = tokens_to_decode[i][topi[j]]\n",
        "          if len(top_word) >= 3 and \"virus\" not in top_word \\\n",
        "             and \"corona\" not in top_word and printed<3:\n",
        "            printed+=1\n",
        "            #print(top_word)\n",
        "            top_words.append(top_word)\n",
        "        all_top_words.append(top_words)\n",
        "\n",
        "    return all_top_words\n",
        "\n",
        "layer_ = 6\n",
        "head_ = 8\n",
        "print()\n",
        "print(layer_, head_)\n",
        "print()\n",
        "train_0 = [\"Ending coronavirus lockdowns will be a dangerous process of trial and error\", \n",
        "           \"Antibody tests suggest that coronavirus infections vastly exceed official counts\",\n",
        "           \"Study Finds Nearly Everyone Who Recovers From COVID-19 Makes Coronavirus Antibodies\",\n",
        "           \"New Zealand eliminates COVID-19\",\n",
        "           \"Evidence that Vitamin D Supplementation Could Reduce Risk of Influenza and COVID-19 Infections and Deaths\"]\n",
        "aw, ids_to_decode, tokens_to_decode = run_attn_examples(train_0, layer=layer_, head=head_, bs=128)\n",
        "top_words = prepare_data(aw, ids_to_decode, tokens_to_decode)\n",
        "print(top_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "chardet",
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "chardet"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "01/23/2021 03:38:10 - INFO - __main__ -   device: cpu, n_gpu 0\n",
            "01/23/2021 03:38:10 - INFO - pytorch_pretrained_bert.modeling -   loading archive file /content/drive/MyDrive/misinformation-NLP/models/BERT-SCIFACT-UNCASED\n",
            "01/23/2021 03:38:10 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "01/23/2021 03:38:14 - INFO - bertviz.bertviz.pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "01/23/2021 03:38:15 - INFO - bertviz.bertviz.pytorch_pretrained_bert.modeling -   loading archive file /content/drive/MyDrive/misinformation-NLP/models/BERT-SCIFACT-UNCASED\n",
            "01/23/2021 03:38:15 - INFO - bertviz.bertviz.pytorch_pretrained_bert.modeling -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "01/23/2021 03:38:17 - INFO - bertviz.bertviz.pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 1426.83it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "6 8\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.38s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 2605.16it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[['error', 'dangerous', 'ending'], ['counts', 'tests', 'suggest'], ['study', 'finds', 'makes'], ['eliminate', 'zealand', 'new'], ['deaths', 'could', 'reduce']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3npi0N9z9ckL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c57b19ca-7097-4e37-f175-6c5a1ea6b5b9"
      },
      "source": [
        "import re\n",
        "\n",
        "train_0 = [c.replace(\"\\\"\", \"\") for c in list(claims['claim'])]\n",
        "aw, ids_to_decode, tokens_to_decode = run_attn_examples(train_0, layer=layer_, head=head_, bs=128)\n",
        "top_words = prepare_data(aw, ids_to_decode, tokens_to_decode)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 552/552 [00:00<00:00, 3452.22it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:02<00:00, 24.59s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 552/552 [00:00<00:00, 20996.05it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvbINX1A_7VX"
      },
      "source": [
        "outfile = '/content/drive/MyDrive/misinformation-NLP/results/additional_1k.csv'\n",
        "\n",
        "sources = list(claims['source_url'])\n",
        "flairs = list(claims['flair'])\n",
        "with open(outfile, \"w\") as out_fp:\n",
        "  writer = csv.writer(out_fp, delimiter=\",\")\n",
        "  writer.writerow(['claim', 'keyword', \n",
        "                   'src_url', 'flair'])\n",
        "  for i in range(0, len(top_words)):\n",
        "    for j in range(0, len(top_words[i])):\n",
        "      claim = train_0[i]\n",
        "      keyword = top_words[i][j]\n",
        "      # mask = claim.lower().replace(keyword, \"_\")\n",
        "      # mask = re.sub(r'_\\w*', '_', mask)\n",
        "      # #print(claims[i])\n",
        "      # #print(claim)\n",
        "      src_url = sources[i]\n",
        "      flair = flairs[i]\n",
        "      writer.writerow([claim, keyword, src_url, flair])\n",
        "\n",
        "# below is the code to mask the claims for the ILM appraoch\n",
        "# with open(outfile, \"w\") as out_fp:\n",
        "#   writer = csv.writer(out_fp, delimiter=\",\")\n",
        "#   writer.writerow(['claim', 'keyword', 'mask', \n",
        "#                    'src_url', 'flair'])\n",
        "#   for i in range(0, len(top_words)):\n",
        "#     for j in range(0, len(top_words[i])):\n",
        "#       claim = train_0[i]\n",
        "#       keyword = top_words[i][j]\n",
        "#       mask = claim.lower().replace(keyword, \"_\")\n",
        "#       mask = re.sub(r'_\\w*', '_', mask)\n",
        "#       #print(claims[i])\n",
        "#       #print(claim)\n",
        "#       src_url = sources[i]\n",
        "#       flair = flairs[i]\n",
        "#       writer.writerow([claim, keyword,  mask, src_url, flair])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmV9HP5TGNKp"
      },
      "source": [
        "# Now generate the claims replacing the keywords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muAofSONRrpB",
        "outputId": "3c30b51c-18d7-4d56-c458-2ebd1a1c111f"
      },
      "source": [
        "# trying the Roberta pre-trained on CORD19\n",
        "from transformers import pipeline\n",
        "from transformers import RobertaTokenizerFast, RobertaForMaskedLM\n",
        "\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"amoux/roberta-cord19-1M7k\")\n",
        "model = RobertaForMaskedLM.from_pretrained(\"amoux/roberta-cord19-1M7k\")\n",
        "\n",
        "fillmask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "text = \"Lung infiltrates cause significant morbidity and mortality in immunocompromised patients.\"\n",
        "masked_text = text.replace(\"patients\", tokenizer.mask_token)\n",
        "predictions = fillmask(masked_text, top_k=3)\n",
        "predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.6273620128631592,\n",
              "  'sequence': '<s>Lung infiltrates cause significant morbidity and mortality in immunocompromised patients.</s>',\n",
              "  'token': 660,\n",
              "  'token_str': 'Ä patients'},\n",
              " {'score': 0.19800478219985962,\n",
              "  'sequence': '<s>Lung infiltrates cause significant morbidity and mortality in immunocompromised individuals.</s>',\n",
              "  'token': 1868,\n",
              "  'token_str': 'Ä individuals'},\n",
              " {'score': 0.022069646045565605,\n",
              "  'sequence': '<s>Lung infiltrates cause significant morbidity and mortality in immunocompromised animals.</s>',\n",
              "  'token': 1471,\n",
              "  'token_str': 'Ä animals'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBOdDMSeayfe",
        "outputId": "b3f16721-4306-4998-fe73-4cafed397907"
      },
      "source": [
        "text = \"Probability of dying of COVID-19 in hospital doubled as L.A. County 's death toll\"\n",
        "masked_text = text.replace(\"increased\", tokenizer.mask_token)\n",
        "predictions = fillmask(masked_text, top_k=3)\n",
        "predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.8134004473686218,\n",
              "  'sequence': \"<s>Probability of dying of COVID-19 in hospital doubled as L.A. County's death toll'?</s>\",\n",
              "  'token': 11,\n",
              "  'token_str': \"'\"},\n",
              " {'score': 0.027315134182572365,\n",
              "  'sequence': '<s>Probability of dying of COVID-19 in hospital doubled as L.A. County\\'s death toll\"?</s>',\n",
              "  'token': 6,\n",
              "  'token_str': '\"'},\n",
              " {'score': 0.01859157904982567,\n",
              "  'sequence': \"<s>Probability of dying of COVID-19 in hospital doubled as L.A. County's death toll''?</s>\",\n",
              "  'token': 4149,\n",
              "  'token_str': \"''\"}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "F-jS0dzDRtRV",
        "outputId": "03c594c6-6cb3-4c06-aed7-8adcd9aa995c"
      },
      "source": [
        "#fname = '/content/drive/MyDrive/misinformation-NLP/tmp/sample.csv'\n",
        "fname = '/content/drive/MyDrive/misinformation-NLP/results/additional_1k.csv'\n",
        "claims = pd.read_csv(fname)\n",
        "\n",
        "#output_fname = \"roberta_allValid_attention_reddit.csv\"\n",
        "#output_fname = \"roberta_allValid_attention_pushshift10K.csv\"\n",
        "#output_fname = \"roberta_PushALLValid_attention.csv\"\n",
        "# output_fname = 'roberta-CORD19-SAMPLE.csv'\n",
        "\n",
        "\n",
        "# claims = claims[:5]\n",
        "\n",
        "claims"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>claim</th>\n",
              "      <th>keyword</th>\n",
              "      <th>src_url</th>\n",
              "      <th>flair</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Air Pollution Is Associated with COVID-19 Inci...</td>\n",
              "      <td>with</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7...</td>\n",
              "      <td>General</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Air Pollution Is Associated with COVID-19 Inci...</td>\n",
              "      <td>associated</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7...</td>\n",
              "      <td>General</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Air Pollution Is Associated with COVID-19 Inci...</td>\n",
              "      <td>austria</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7...</td>\n",
              "      <td>General</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Fauci says Trump policies very likely led to d...</td>\n",
              "      <td>likely</td>\n",
              "      <td>https://www.scmp.com/news/world/united-states-...</td>\n",
              "      <td>General</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Fauci says Trump policies very likely led to d...</td>\n",
              "      <td>deaths</td>\n",
              "      <td>https://www.scmp.com/news/world/united-states-...</td>\n",
              "      <td>General</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1643</th>\n",
              "      <td>Iran says coronavirus has spread to several ci...</td>\n",
              "      <td>says</td>\n",
              "      <td>https://edition.cnn.com/asia/live-news/coronav...</td>\n",
              "      <td>Government Agency</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1644</th>\n",
              "      <td>Iran says coronavirus has spread to several ci...</td>\n",
              "      <td>cities</td>\n",
              "      <td>https://edition.cnn.com/asia/live-news/coronav...</td>\n",
              "      <td>Government Agency</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1645</th>\n",
              "      <td>China Reports Smallest Number Of New COVID-19 ...</td>\n",
              "      <td>but</td>\n",
              "      <td>https://khn.org/morning-breakout/china-reports...</td>\n",
              "      <td>News Article</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1646</th>\n",
              "      <td>China Reports Smallest Number Of New COVID-19 ...</td>\n",
              "      <td>doesn</td>\n",
              "      <td>https://khn.org/morning-breakout/china-reports...</td>\n",
              "      <td>News Article</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1647</th>\n",
              "      <td>China Reports Smallest Number Of New COVID-19 ...</td>\n",
              "      <td>reports</td>\n",
              "      <td>https://khn.org/morning-breakout/china-reports...</td>\n",
              "      <td>News Article</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1648 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  claim  ...              flair\n",
              "0     Air Pollution Is Associated with COVID-19 Inci...  ...            General\n",
              "1     Air Pollution Is Associated with COVID-19 Inci...  ...            General\n",
              "2     Air Pollution Is Associated with COVID-19 Inci...  ...            General\n",
              "3     Fauci says Trump policies very likely led to d...  ...            General\n",
              "4     Fauci says Trump policies very likely led to d...  ...            General\n",
              "...                                                 ...  ...                ...\n",
              "1643  Iran says coronavirus has spread to several ci...  ...  Government Agency\n",
              "1644  Iran says coronavirus has spread to several ci...  ...  Government Agency\n",
              "1645  China Reports Smallest Number Of New COVID-19 ...  ...       News Article\n",
              "1646  China Reports Smallest Number Of New COVID-19 ...  ...       News Article\n",
              "1647  China Reports Smallest Number Of New COVID-19 ...  ...       News Article\n",
              "\n",
              "[1648 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUJoWgdRSFiX",
        "outputId": "640e7e7b-c6a3-4df2-e383-d7e99774e52e"
      },
      "source": [
        "dir = '/content/drive/MyDrive/misinformation-NLP/results/'\n",
        "data = []\n",
        "i=0\n",
        "for claim, keyword, source, flair in tqdm(zip(claims['claim'], \n",
        "                                                    claims['keyword'], \n",
        "                                                    claims['src_url'],\n",
        "                                                    claims['flair'])):\n",
        "    i+=1\n",
        "    if claim[-1] != \".\":\n",
        "      # if there is no . at the end and we are replacing last word, the model will just predict a punctuation mark\n",
        "      claim = claim +  \".\"\n",
        "\n",
        "    masked_text = claim.lower().replace(keyword, tokenizer.mask_token)\n",
        "    #print(masked_text)\n",
        "\n",
        "    try:\n",
        "      suggs = fillmask(masked_text, top_k=10) #CHANGED TO 10\n",
        "      suggs = [sent['sequence'].replace(\"<s>\", \"\").replace(\"</s>\", \"\") for sent in suggs]\n",
        "      suggs = set(suggs)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "    for sug in suggs:\n",
        "        if sug != claim.lower():\n",
        "          data.append([claim, keyword, sug, source, flair])\n",
        "\n",
        "    if i%100==0:\n",
        "        all_fakes = pd.DataFrame(data, columns=['claim', 'keyword', 'false_claim', 'source', 'flair'])\n",
        "        all_fakes.to_csv(dir+output_fname)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1648it [03:51,  7.11it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9VUJqk8hbeS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}