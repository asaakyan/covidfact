# -*- coding: utf-8 -*-
"""experiments.ipynb

Automatically generated by Colaboratory.

Original file is located at
		https://colab.research.google.com/drive/1uPusgXR4IjCjM_jPl-9NP0DTdE_qp04s
"""

import pandas as pd
import numpy as np

from tqdm import tqdm
import ast
import os
import json
# from google.colab import drive
# drive.mount('/content/drive')
dir = "/content/drive/My Drive/misinformation-NLP/"
# os.listdir(dir)

# outfile = "/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/DATASET_4k_train.json"
# with open(outfile, "r") as outfile:  
#     train_data = json.load(outfile) 
# print(len(train_data))

# outfile = "/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/DATASET_4k_test.json"
# with open(outfile, "r") as outfile:  
#     test_data = json.load(outfile) 
# print(len(test_data))

# outfile = "/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/DATASET_4k_dev.json"
# with open(outfile, "r") as outfile:  
#     dev_data = json.load(outfile) 
# print(len(dev_data))

#!pip install allennlp allennlp-models

os.environ["CUDA_VISIBLE_DEVICES"]='0'
from allennlp.predictors.predictor import Predictor
import allennlp_models.pair_classification
predictor = Predictor.from_path("https://storage.googleapis.com/allennlp-public-models/mnli_roberta-2020.06.09.tar.gz", "textual_entailment")
predictor._model = predictor._model.cuda()

test5 = pd.read_csv('/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/rte-covid-tuhin/evidence_recall/test1_5.tsv', sep='\t')
test5.rename(columns={'entailment': 'label'}, inplace=True)
# test3 = pd.read_csv('/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/rte-covid-tuhin/test1_3.tsv', sep='\t')
# test3
test1 = pd.read_csv('/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/rte-covid-tuhin/evidence_recall/test1_1.tsv', sep='\t')
test1.rename(columns={'entailment': 'label'}, inplace=True)
test1

# !git clone https://github.com/pytorch/fairseq
# os.chdir('fairseq')
# !pip install ./ 
# !wget https://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz
# !tar -xzvf roberta.large.tar.gz

from fairseq.models.roberta import RobertaModel
roberta = RobertaModel.from_pretrained('/content/fairseq/roberta.large', checkpoint_file='model.pt')
roberta.eval()  # disable dropout (or leave in train mode to finetune)

def get_contra_score(claim, evidence):
	MAX_TOKENS = 512
	sentence_tok = roberta.encode(evidence)
	if len(sentence_tok) > MAX_TOKENS:
			evidence = roberta.decode(sentence_tok[:510])
			print(evidence)
			#continue
	return predictor.predict( hypothesis=claim, premise=evidence)['probs'][:2]

#get_contra_score("Stillbirth rate rises dramatically during pandemic", "stillbirth rate drops dramatically during pandemic")

from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score

def get_PR(og_df):
	df = og_df.copy()
	yhat = []
	ytrue = []
	ytrue_df = list(df['label'])
	for yt in ytrue_df:
		if yt == 'entailment':
			ytrue.append(0)
		elif yt == 'not_entailment':
			ytrue.append(1)
	
	tqdm.pandas()
	df['pred'] = df.progress_apply( lambda x: np.argmax(get_contra_score( x['sentence2'], 
																															x['sentence1']) ), axis=1 )
	yhat = np.array(df['pred'])

	print(precision_score(ytrue, yhat), recall_score(ytrue, yhat), f1_score(ytrue, yhat), accuracy_score(ytrue, yhat))
	return precision_score(ytrue, yhat), recall_score(ytrue, yhat), f1_score(ytrue, yhat), accuracy_score(ytrue, yhat), ytrue, yhat


pr5, rec5, f15, acc5, ytrue5, yhat5 = get_PR(test5)

# pr1, rec1, f11, acc1 = get_PR(test1)

# get_PR(test3)

def convert_to_label(x):
	if x == 1:
		return "not_entailment"
	elif x == 0:
		return "entailment"

covidfever = [[convert_to_label(x),convert_to_label(y)] for x,y in zip(yhat5, ytrue5)]
covidfever_df = pd.DataFrame(covidfever, columns=['predicted', 'gold'])
covidfever_df.to_csv('/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/covidfever.tsv', index=True, sep='\t', index_label='index')

get_PR(test1)

test1_0 = pd.read_csv('/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/rte-covid-tuhin/test1.tsv', sep='\t')
#test1_0.rename(columns={'entailment': 'label'}, inplace=True)
get_PR(test1_0)

count = 0
c = 0
for x,y in zip(open('/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/rte-covid-tuhin/evidence_recall/test1_5_recall.tsv'),
							 open('/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/covidfever.tsv')):
	
	x = x.strip().split('\t')[-1]
	y = y.strip().split('\t')
	#print(x, y)
	#print(y[1], y[2])
	if y[1]==y[2] and x=='1':
		count = count+1
	c = c+1
print(float(count)/float(c))

`` from scipy import stats
from tqdm import tqdm 

ytrue = []
yhat_rationale =  []
yhat_corpus = []

for dev in tqdm(dev_data):
	#print(dev_data[dev])
	label = dev_data[dev]['label']
	evidence = dev_data[dev]['evidence']

	if label == 'SUPPORTS':
		ytrue.append(0)
	else:
		ytrue.append(1)
	
	#print(evidence)

	corpus_evidence = " ".join(evidence)
	# print(corpus_evidence)
	# break
	corpus_entailment = get_contra_score(dev, corpus_evidence)
	#print(entailment)
	yhat_corpus.append(np.argmax(corpus_entailment))

	sent_entailment = []
	for sent in evidence:
		sent_entailment.append(np.argmax(get_contra_score(dev, sent)))
	yhat_rationale.append(stats.mode(sent_entailment)[0][0])
	# print(sent_entailment)
	# print(stats.mode(sent_entailment)[0][0])
	#break

from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score


print(precision_score(ytrue, yhat_corpus))
print(precision_score(ytrue, yhat_rationale))

print(recall_score(ytrue, yhat_corpus))
print(recall_score(ytrue, yhat_rationale))

print(f1_score(ytrue, yhat_corpus))
print(f1_score(ytrue, yhat_rationale))

print(accuracy_score(ytrue, yhat_corpus))
print(accuracy_score(ytrue, yhat_rationale))

ytrue = []
yhat_rationale =  []
yhat_corpus = []

for dev in tqdm(test_data):
	#print(dev_data[dev])
	label = test_data[dev]['label']
	evidence = test_data[dev]['evidence']

	if label == 'SUPPORTS':
		ytrue.append(0)
	else:
		ytrue.append(1)
	
	#print(evidence)

	corpus_evidence = " ".join(evidence)
	# print(corpus_evidence)
	# break
	corpus_entailment = get_contra_score(dev, corpus_evidence)
	#print(entailment)
	yhat_corpus.append(np.argmax(corpus_entailment))

	sent_entailment = []
	for sent in evidence:
		sent_entailment.append(np.argmax(get_contra_score(dev, sent)))
	yhat_rationale.append(stats.mode(sent_entailment)[0][0])
	# print(sent_entailment)
	# print(stats.mode(sent_entailment)[0][0])
	#break

print(precision_score(ytrue, yhat_corpus))
print(precision_score(ytrue, yhat_rationale))

print(recall_score(ytrue, yhat_corpus))
print(recall_score(ytrue, yhat_rationale))

print(f1_score(ytrue, yhat_corpus))
print(f1_score(ytrue, yhat_rationale))

print(accuracy_score(ytrue, yhat_corpus))
print(accuracy_score(ytrue, yhat_rationale))

# data = test_data
# output_file_path = '/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/test.tsv'
# import csv
# # with open(output_file_path, "w") as out_fp:
# #   writer = csv.writer(out_fp, delimiter="\t")
# #   writer.writerow(['index',	'sentence1',	'sentence2',	'label'])
# with open(output_file_path, "w") as out_fp:
#   writer = csv.writer(out_fp, delimiter="\t")
#   writer.writerow(['index',	'sentence1',	'sentence2'])
#   i=0
#   for claim in data:
		
#     writer.writerow([i,	" ".join(data[claim]['evidence']), claim])
#     i+=1

#     # if data[claim]['label'] == 'SUPPORTS':
#     #   writer.writerow([i,	" ".join(data[claim]['evidence']), claim])
#     # else:
#     #   writer.writerow([i,	" ".join(data[claim]['evidence']), claim])

dev_data

# !git clone https://github.com/pytorch/fairseq
# os.chdir('fairseq')
# !pip install ./

# !wget https://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz
# !tar -xzvf roberta.large.tar.gz
# # Load the model in fairseq
# from fairseq.models.roberta import RobertaModel
# roberta = RobertaModel.from_pretrained('/content/fairseq/roberta.large', checkpoint_file='model.pt')
# roberta.eval()  # disable dropout (or leave in train mode to finetune)
# tokens = roberta.encode('Hello world!')
# assert tokens.tolist() == [0, 31414, 232, 328, 2]
# roberta.decode(tokens)

# Load the model in fairseq
from fairseq.models.roberta import RobertaModel
roberta = RobertaModel.from_pretrained('/content/fairseq/roberta.large', checkpoint_file='model.pt')
roberta.eval()  # disable dropout (or leave in train mode to finetune)

tokens = roberta.encode('Hello world!')
# assert tokens.tolist() == [0, 31414, 232, 328, 2]
# roberta.decode(tokens)
tokens[:4]

# output_file_path = '/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/dev.tsv'
MAX_TOKENS = 512

def get_df_data(data):
	df_data = []
	for claim in data:
		sentence1 = " ".join(data[claim]['evidence']).replace('\n', ' ').replace('"', '')
		# sentence_tok = roberta.encode(sentence1)
		# if len(sentence_tok) > MAX_TOKENS:
		#   sentence1 = roberta.decode(sentence_tok[:512])
		#   print(sentence1)
		#   continue
		if data[claim]['label'] == 'SUPPORTS':
			df_data.append([sentence1 , claim, 'entailment'])
		else:
			df_data.append([sentence1 , claim, 'not_entailment'])

	data_df = pd.DataFrame(df_data, columns=['sentence1', 'sentence2', 'label'])
	display(data_df.head(3))
	return data_df

#dev_df = get_df_data(dev_data)
#dev_df.to_csv('/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/dev.tsv', index=True, sep='\t', index_label='index')



# # output_file_path = '/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/dev.tsv'
# MAX_TOKENS = 512

# def get_df_data(data):
#   df_data = []
#   for claim in data:
#     sentence1 = " ".join(data[claim]['evidence']).replace('\n', ' ').replace('"', '')
#     sentence_tok = roberta.encode(sentence1)
#     if len(sentence_tok) > MAX_TOKENS:
#       sentence1 = roberta.decode(sentence_tok[:512])
#       print(sentence1)
#       continue
#     if data[claim]['label'] == 'SUPPORTS':
#       df_data.append([sentence1 , claim, 'entailment'])
#     else:
#       df_data.append([sentence1 , claim, 'not_entailment'])

#   data_df = pd.DataFrame(df_data, columns=['sentence1', 'sentence2', 'label'])
#   display(data_df.head(3))
#   return data_df

# #dev_df = get_df_data(dev_data)
# #dev_df.to_csv('/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/dev.tsv', index=True, sep='\t', index_label='index')

train_df = get_df_data(train_data)
train_df.to_csv('/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/train.tsv', index=True, sep='\t', index_label='index')

# test_df = get_df_data(test_data)
# test_df[['sentence1', 'sentence2']].to_csv('/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/test.tsv', index=True, sep='\t', index_label='index')
# test_df[['sentence1', 'sentence2', 'label']].to_csv('/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/test1.tsv', index=True, sep='\t', index_label='index')
# labels = test_df['label']
# labels.to_csv('/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/test_labels.tsv', index=True, sep='\t', index_label='index')

output_file_path = '/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/dev.tsv'

dev_df_data = []
for claim in dev_data:
	
	if dev_data[claim]['label'] == 'SUPPORTS':
		dev_df_data.append([" ".join(dev_data[claim]['evidence']).replace('\n', ' ').replace('"', '') , claim, 'entailment'])
	else:
		dev_df_data.append([" ".join(dev_data[claim]['evidence']).replace('\n', ' ').replace('"', '') , claim, 'not_entailment'])

dev_df = pd.DataFrame(dev_df_data, columns=['sentence1', 'sentence2', 'entailment'])
dev_df
dev_df.to_csv('/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/dev.tsv', index=True, sep='\t', index_label='index')

output_file_path = '/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/dev.tsv'

dev_df_data = []
for claim in test_data:
	
	if test_data[claim]['label'] == 'SUPPORTS':
		dev_df_data.append([" ".join(test_data[claim]['evidence']).replace('\n', ' ').replace('"', '') , claim, 'entailment'])
	else:
		dev_df_data.append([" ".join(test_data[claim]['evidence']).replace('\n', ' ').replace('"', '') , claim, 'not_entailment'])

dev_df = pd.DataFrame(dev_df_data, columns=['sentence1', 'sentence2', 'entailment'])
dev_df
dev_df.to_csv('/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/dev.tsv', index=True, sep='\t', index_label='index')

!pip install json-lines
import json_lines

# %%shell
# wget https://scifact.s3-us-west-2.amazonaws.com/release/2020-12-17/claims_with_citances.jsonl -P data

# import json_lines

# fname = '/content/data/claims_with_citances.jsonl'
# with open(fname, 'rb') as f: # opening file in binary(rb) mode    
#   for item in json_lines.reader(f):
#     print(item['claims'])
#     print()
#     break
import json 
def load_jsonl(input_path) -> list:
		"""
		Read list of objects from a JSON lines file.
		"""
		data = []
		with open(input_path, 'r', encoding='utf-8') as f:
				for line in f:
						data.append(json.loads(line.rstrip('\n|\r')))
		print('Loaded {} records from {}'.format(len(data), input_path))
		return data

# output_file_path = './out.tsv'
# import csv
# with open(output_file_path, "w") as out_fp:
#   writer = csv.writer(out_fp, delimiter="\t")
#   writer.writerow(['index',	'sentence1',	'sentence2',	'label'])

corpus_fname = '/content/drive/MyDrive/misinformation-NLP/scifact/corpus.jsonl'
scifact_corpus = load_jsonl(corpus_fname)
len(scifact_corpus)

def find_rationale(id, sents):
	evid = []
	for c in scifact_corpus:
		if c['doc_id']==id:
			evid = []
			for ev_idx in sents:
				evid.append(c['abstract'][ev_idx])
	return evid

# {'cited_doc_ids': [2867345],
#   'claim': 'In British Men, haplogroup I decreases risk of cardiovascular disease.',
#   'evidence': {'2867345': [{'label': 'CONTRADICT', 'sentences': [7]}]},
#   'id': 563},
find_rationale(13734012, [4])



input_fname = '/content/drive/MyDrive/misinformation-NLP/scifact/claims_train.jsonl'
scifact_claims = load_jsonl(input_fname)

input_fname = '/content/drive/MyDrive/misinformation-NLP/scifact/claims_train.jsonl'


output_file_path = '/content/drive/MyDrive/misinformation-NLP/tmp/glue_data/RTE/train.tsv'

import csv
with open(output_file_path, "w") as out_fp:
	writer = csv.writer(out_fp, delimiter="\t")
	writer.writerow(['index',	'sentence1',	'sentence2',	'label'])

	scifact_claims = load_jsonl(input_fname)
	i=0
	for row in scifact_claims:

		#print(row)
		claim = row['claim']
		#print(claim)
		evidence = row['evidence']
		if len(evidence) > 0:
			for doc in evidence:
				label = evidence[doc][0]['label']
				if label == 'CONTRADICT':
					label = 'not_entailment'
				elif label == 'SUPPORT':
					label = 'entailment'
				else:
					continue 
				#print(label)
				doc_id = int(doc)
				all_sents = []
				for rationale in evidence[doc]:
					rationale_sents = rationale['sentences']
					all_sents += rationale_sents
				doc_rationale = find_rationale(doc_id, all_sents)
				doc_rationale = " ".join(doc_rationale).strip().replace("\n", '')
				writer.writerow([i, doc_rationale, claim, label])
				i+=1
				#print(doc_rationale)
			#print(all_sents, doc)

			#break



# scifact_claims_dict = {}
# for sc in scifact_claims:
#   scifact_claims_dict[]
#scifact_corpus = load_jsonl(corpus_fname)

#scifact_claims

input_fname = '/content/drive/MyDrive/misinformation-NLP/scifact/claims_dev.jsonl'
corpus_fname = '/content/drive/MyDrive/misinformation-NLP/scifact/corpus.jsonl'

output_file_path = '/content/drive/MyDrive/misinformation-NLP/tmp/glue_data/RTE/dev.tsv'
import csv
with open(output_file_path, "w") as out_fp:
	writer = csv.writer(out_fp, delimiter="\t")
	writer.writerow(['index',	'sentence1',	'sentence2',	'label'])

	scifact_claims = load_jsonl(input_fname)
	i=0
	for row in scifact_claims:

		#print(row)
		claim = row['claim']
		#print(claim)
		evidence = row['evidence']
		if len(evidence) > 0:
			for doc in evidence:
				label = evidence[doc][0]['label']
				if label == 'CONTRADICT':
					label = 'not_entailment'
				elif label == 'SUPPORT':
					label = 'entailment'
				else:
					continue 
				#print(label)
				doc_id = int(doc)
				all_sents = []
				for rationale in evidence[doc]:
					rationale_sents = rationale['sentences']
					all_sents += rationale_sents
				doc_rationale = find_rationale(doc_id, all_sents)
				doc_rationale = " ".join(doc_rationale).strip().replace("\n", '')
				
				writer.writerow([i, doc_rationale, claim, label])
				i+=1
				#print(doc_rationale)
			#print(all_sents, doc)

			#break



# scifact_claims_dict = {}
# for sc in scifact_claims:
#   scifact_claims_dict[]
#scifact_corpus = load_jsonl(corpus_fname)

#scifact_claims

# input_fname = '/content/drive/MyDrive/misinformation-NLP/scifact/claims_dev.jsonl'
# corpus_fname = '/content/drive/MyDrive/misinformation-NLP/scifact/corpus.jsonl'

# output_file_path = './dev.tsv'
# import csv
# with open(output_file_path, "w") as out_fp:
#   writer = csv.writer(out_fp, delimiter="\t")
#   writer.writerow(['index',	'sentence1',	'sentence2',	'label'])

#   scifact_claims = load_jsonl(input_fname)
#   i=0
#   for row in scifact_claims:

#     #print(row)
#     claim = row['claim']
#     #print(claim)
#     evidence = row['evidence']
#     if len(evidence) > 0:
#       for doc in evidence:
#         label = evidence[doc][0]['label']
#         if label == 'CONTRADICT':
#           label = 'not_entailment'
#         elif label == 'SUPPORT':
#           label = 'entailment'
#         else:
#           continue 
#         #print(label)
#         doc_id = int(doc)
#         all_sents = []
#         for rationale in evidence[doc]:
#           rationale_sents = rationale['sentences']
#           all_sents += rationale_sents
#         doc_rationale = find_rationale(doc_id, all_sents)
#         doc_rationale = " ".join(doc_rationale).strip().replace("\n", '')
				
#         writer.writerow([i, doc_rationale, claim, label])
#         i+=1
#         #print(doc_rationale)
#       #print(all_sents, doc)

#       #break



# # scifact_claims_dict = {}
# # for sc in scifact_claims:
# #   scifact_claims_dict[]
# #scifact_corpus = load_jsonl(corpus_fname)

# #scifact_claims

# outfile = "/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/DATASET_4k_train.json"
# with open(outfile, "r") as outfile:  
#     train_data = json.load(outfile) 
# print(len(train_data))

# outfile = "/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/DATASET_4k_test.json"
# with open(outfile, "r") as outfile:  
#     test_data = json.load(outfile) 
# print(len(test_data))

# outfile = "/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/DATASET_4k_dev.json"
# with open(outfile, "r") as outfile:  
#     dev_data = json.load(outfile) 
# print(len(dev_data))

# data = test_data
# output_file_path = '/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/test.tsv'
# import csv
# # with open(output_file_path, "w") as out_fp:
# #   writer = csv.writer(out_fp, delimiter="\t")
# #   writer.writerow(['index',	'sentence1',	'sentence2',	'label'])
# with open(output_file_path, "w") as out_fp:
#   writer = csv.writer(out_fp, delimiter="\t")
#   writer.writerow(['index',	'sentence1',	'sentence2',	'label'])
#   writer = csv.writer(out_fp, delimiter="\t")
#   i=0
#   for claim in data:
#     i+=1
#     if data[claim]['label'] == 'SUPPORTS':
#       writer.writerow([i,	" ".join(data[claim]['evidence']), claim,	'entailment'])
#     else:
#       writer.writerow([i,	" ".join(data[claim]['evidence']), claim,	'not_entailment'])

# data = train_data
# output_file_path = '/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/train.tsv'
# import csv
# # with open(output_file_path, "w") as out_fp:
# #   writer = csv.writer(out_fp, delimiter="\t")
# #   writer.writerow(['index',	'sentence1',	'sentence2',	'label'])
# with open(output_file_path, "w") as out_fp:
#   writer = csv.writer(out_fp, delimiter="\t")
#   writer.writerow(['index',	'sentence1',	'sentence2',	'label'])
#   writer = csv.writer(out_fp, delimiter="\t")
#   i=0
#   for claim in data:
#     i+=1
#     if data[claim]['label'] == 'SUPPORTS':
#       writer.writerow([i,	" ".join(data[claim]['evidence']), claim,	'entailment'])
#     else:
#       writer.writerow([i,	" ".join(data[claim]['evidence']), claim,	'not_entailment'])


# data = dev_data
# output_file_path = '/content/drive/MyDrive/misinformation-NLP/tmp/DATASET/dev.tsv'
# import csv
# # with open(output_file_path, "w") as out_fp:
# #   writer = csv.writer(out_fp, delimiter="\t")
# #   writer.writerow(['index',	'sentence1',	'sentence2',	'label'])
# with open(output_file_path, "w") as out_fp:
#   writer = csv.writer(out_fp, delimiter="\t")
#   writer.writerow(['index',	'sentence1',	'sentence2',	'label'])
#   writer = csv.writer(out_fp, delimiter="\t")
#   i=0
#   for claim in data:
#     i+=1
#     if data[claim]['label'] == 'SUPPORTS':
#       writer.writerow([i,	" ".join(data[claim]['evidence']), claim,	'entailment'])
#     else:
#       writer.writerow([i,	" ".join(data[claim]['evidence']), claim,	'not_entailment'])

# !git clone https://github.com/pytorch/fairseq
# os.chdir('fairseq')
# !pip install ./

# %%shell
# git clone https://github.com/pytorch/fairseq.git
# cd fairseq
# pip install --editable ./

# import sys
# sys.path.insert(0, '/content/fairseq')

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# 
# /content/fairseq/examples/roberta/preprocess_GLUE_tasks.sh /content/drive/MyDrive/misinformation-NLP/tmp/glue_data RTE

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# 
# TOTAL_NUM_UPDATES=2036  # 10 epochs through RTE for bsz 16
# WARMUP_UPDATES=122      # 6 percent of the number of updates
# LR=2e-05                # Peak LR for polynomial LR scheduler.
# NUM_CLASSES=1
# MAX_SENTENCES=16        # Batch size.
# ROBERTA_PATH=/roberta_scifact.pt
# 
# CUDA_VISIBLE_DEVICES=0 fairseq-train RTE-bin/ \
#     --restore-file $ROBERTA_PATH \
#     --max-positions 512 \
#     --batch-size $MAX_SENTENCES \
#     --max-tokens 4400 \
#     --task sentence_prediction \
#     --reset-optimizer --reset-dataloader --reset-meters \
#     --required-batch-size-multiple 1 \
#     --init-token 0 --separator-token 2 \
#     --arch roberta_large \
#     --criterion sentence_prediction \
#     --num-classes $NUM_CLASSES \
#     --dropout 0.1 --attention-dropout 0.1 \
#     --weight-decay 0.1 --optimizer adam --adam-betas "(0.9, 0.98)" --adam-eps 1e-06 \
#     --clip-norm 0.0 \
#     --lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_NUM_UPDATES --warmup-updates $WARMUP_UPDATES \
#     --fp16 --fp16-init-scale 4 --threshold-loss-scale 1 --fp16-scale-window 128 \
#     --max-epoch 10 \
#     --find-unused-parameters \
#     --best-checkpoint-metric accuracy --maximize-best-checkpoint-metric;

